#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒæ•°æ®è´¨é‡æ£€æŸ¥å’Œæ”¹è¿›å·¥å…·
ç”¨äºåˆ†æå’Œæå‡é‚®ä»¶åˆ†ç±»è®­ç»ƒæ•°æ®çš„è´¨é‡
"""

import pandas as pd
import numpy as np
import re
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns

class DataQualityChecker:
    def __init__(self):
        self.df = None
        self.load_data()
    
    def load_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        try:
            self.df = pd.read_csv('emails.csv')
            print(f"âœ… å·²åŠ è½½ {len(self.df)} æ¡è®­ç»ƒæ•°æ®")
        except FileNotFoundError:
            print("âŒ æœªæ‰¾åˆ° emails.csv æ–‡ä»¶!")
            exit(1)
    
    def basic_statistics(self):
        """åŸºç¡€ç»Ÿè®¡åˆ†æ"""
        print("\nğŸ“Š åŸºç¡€ç»Ÿè®¡åˆ†æ")
        print("=" * 60)
        
        # æ•°æ®æ¦‚è§ˆ
        print(f"æ€»æ•°æ®é‡: {len(self.df)} æ¡")
        print(f"æ•°æ®åˆ—: {list(self.df.columns)}")
        
        # ç¼ºå¤±å€¼æ£€æŸ¥
        missing_data = self.df.isnull().sum()
        if missing_data.sum() > 0:
            print(f"\nâš ï¸  ç¼ºå¤±å€¼:")
            for col, count in missing_data.items():
                if count > 0:
                    print(f"  {col}: {count} æ¡ ({count/len(self.df)*100:.1f}%)")
        else:
            print(f"\nâœ… æ— ç¼ºå¤±å€¼")
        
        # é‡å¤æ•°æ®æ£€æŸ¥
        duplicates = self.df.duplicated(['subject', 'body']).sum()
        if duplicates > 0:
            print(f"\nâš ï¸  é‡å¤é‚®ä»¶: {duplicates} æ¡")
            # æ˜¾ç¤ºé‡å¤é‚®ä»¶
            duplicate_emails = self.df[self.df.duplicated(['subject', 'body'], keep=False)]
            for _, row in duplicate_emails.iterrows():
                print(f"  - {row['subject'][:50]}... [{row['label']}]")
        else:
            print(f"\nâœ… æ— é‡å¤é‚®ä»¶")
        
        # æ–‡æœ¬é•¿åº¦åˆ†æ
        self.df['text_length'] = (
            self.df['subject'].fillna('').astype(str) + ' ' + 
            self.df['body'].fillna('').astype(str)
        ).str.len()
        
        print(f"\nğŸ“ æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡é•¿åº¦: {self.df['text_length'].mean():.0f} å­—ç¬¦")
        print(f"  ä¸­ä½æ•°é•¿åº¦: {self.df['text_length'].median():.0f} å­—ç¬¦")
        print(f"  æœ€çŸ­: {self.df['text_length'].min()} å­—ç¬¦")
        print(f"  æœ€é•¿: {self.df['text_length'].max()} å­—ç¬¦")
        
        # æ‰¾å‡ºå¼‚å¸¸çŸ­çš„é‚®ä»¶
        short_emails = self.df[self.df['text_length'] < 30]
        if len(short_emails) > 0:
            print(f"\nâš ï¸  å¼‚å¸¸çŸ­é‚®ä»¶ (<30å­—ç¬¦): {len(short_emails)} æ¡")
            for _, row in short_emails.iterrows():
                print(f"  - {row['subject']} | {row['body'][:30]}... [{row['label']}]")
    
    def label_distribution_analysis(self):
        """æ ‡ç­¾åˆ†å¸ƒåˆ†æ"""
        print("\nğŸ·ï¸  æ ‡ç­¾åˆ†å¸ƒåˆ†æ")
        print("=" * 60)
        
        # è®¡ç®—åˆ†å¸ƒ
        label_counts = self.df['label'].value_counts()
        total = len(self.df)
        
        print("å½“å‰æ ‡ç­¾åˆ†å¸ƒ:")
        for label, count in label_counts.items():
            percentage = (count / total) * 100
            bar = "â–ˆ" * int(percentage / 2)  # ç®€å•çš„æ¡å½¢å›¾
            print(f"  {label:20} {count:3d} ({percentage:5.1f}%) {bar}")
        
        # æ•°æ®å¹³è¡¡æ€§åˆ†æ
        print(f"\nğŸ“ˆ æ•°æ®å¹³è¡¡æ€§åˆ†æ:")
        max_count = label_counts.max()
        min_count = label_counts.min()
        balance_ratio = min_count / max_count
        
        print(f"  æœ€å¤šç±»åˆ«: {label_counts.idxmax()} ({max_count} æ¡)")
        print(f"  æœ€å°‘ç±»åˆ«: {label_counts.idxmin()} ({min_count} æ¡)")
        print(f"  å¹³è¡¡æ¯”ä¾‹: {balance_ratio:.2f} (1.0ä¸ºå®Œå…¨å¹³è¡¡)")
        
        if balance_ratio < 0.5:
            print(f"  âš ï¸  æ•°æ®ä¸å¹³è¡¡ä¸¥é‡ï¼Œå»ºè®®å¢åŠ å°‘æ•°ç±»åˆ«çš„æ ·æœ¬")
        elif balance_ratio < 0.8:
            print(f"  âš ï¸  æ•°æ®ç•¥æœ‰ä¸å¹³è¡¡ï¼Œå¯ä»¥è€ƒè™‘è°ƒæ•´")
        else:
            print(f"  âœ… æ•°æ®åˆ†å¸ƒç›¸å¯¹å¹³è¡¡")
        
        # æ¨èæ ·æœ¬æ•°
        recommended_per_class = max(50, max_count)
        print(f"\nğŸ’¡ å»ºè®®æ¯ä¸ªç±»åˆ«è‡³å°‘æœ‰ {recommended_per_class} ä¸ªæ ·æœ¬:")
        for label, count in label_counts.items():
            needed = max(0, recommended_per_class - count)
            if needed > 0:
                print(f"  {label}: éœ€è¦å¢åŠ  {needed} æ¡")
    
    def text_quality_analysis(self):
        """æ–‡æœ¬è´¨é‡åˆ†æ"""
        print("\nğŸ“ æ–‡æœ¬è´¨é‡åˆ†æ")
        print("=" * 60)
        
        # åˆå¹¶ä¸»é¢˜å’Œæ­£æ–‡
        self.df['full_text'] = (
            self.df['subject'].fillna('').astype(str) + ' ' + 
            self.df['body'].fillna('').astype(str)
        ).str.strip()
        
        # è¯æ±‡ä¸°å¯Œåº¦åˆ†æ
        all_text = ' '.join(self.df['full_text']).lower()
        words = re.findall(r'\b\w+\b', all_text)
        unique_words = set(words)
        
        print(f"è¯æ±‡ç»Ÿè®¡:")
        print(f"  æ€»è¯æ•°: {len(words):,}")
        print(f"  å”¯ä¸€è¯æ•°: {len(unique_words):,}")
        print(f"  è¯æ±‡ä¸°å¯Œåº¦: {len(unique_words)/len(words):.3f}")
        
        # æœ€å¸¸è§è¯æ±‡
        word_freq = Counter(words)
        print(f"\nğŸ” æœ€å¸¸è§è¯æ±‡ (å‰10ä¸ª):")
        for word, freq in word_freq.most_common(10):
            if len(word) > 2:  # è¿‡æ»¤çŸ­è¯
                print(f"  {word}: {freq} æ¬¡")
        
        # æ¯ä¸ªç±»åˆ«çš„ç‰¹å¾è¯
        print(f"\nğŸ·ï¸  å„ç±»åˆ«ç‰¹å¾è¯åˆ†æ:")
        for label in self.df['label'].unique():
            label_texts = self.df[self.df['label'] == label]['full_text']
            label_text = ' '.join(label_texts).lower()
            label_words = re.findall(r'\b\w+\b', label_text)
            label_freq = Counter(label_words)
            
            print(f"\n{label}:")
            # æ‰¾å‡ºè¯¥ç±»åˆ«ä¸­é¢‘æ¬¡é«˜ä½†åœ¨å…¶ä»–ç±»åˆ«ä¸­å°‘è§çš„è¯
            top_words = []
            for word, freq in label_freq.most_common(20):
                if len(word) > 3 and freq >= 2:
                    top_words.append(word)
            
            print(f"  ç‰¹å¾è¯: {', '.join(top_words[:8])}")
    
    def similarity_analysis(self):
        """æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æ"""
        print("\nğŸ” æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æ")
        print("=" * 60)
        
        # ä½¿ç”¨TF-IDFè®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
        try:
            vectorizer = TfidfVectorizer(max_features=500, stop_words='english', ngram_range=(1, 2))
            tfidf_matrix = vectorizer.fit_transform(self.df['full_text'])
            
            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            # æ‰¾å‡ºé«˜ç›¸ä¼¼åº¦çš„é‚®ä»¶å¯¹
            high_similarity_pairs = []
            threshold = 0.7  # ç›¸ä¼¼åº¦é˜ˆå€¼
            
            for i in range(len(self.df)):
                for j in range(i + 1, len(self.df)):
                    if similarity_matrix[i][j] > threshold:
                        high_similarity_pairs.append({
                            'index1': i,
                            'index2': j,
                            'similarity': similarity_matrix[i][j],
                            'label1': self.df.iloc[i]['label'],
                            'label2': self.df.iloc[j]['label'],
                            'subject1': self.df.iloc[i]['subject'][:30],
                            'subject2': self.df.iloc[j]['subject'][:30]
                        })
            
            if high_similarity_pairs:
                print(f"ğŸ” å‘ç° {len(high_similarity_pairs)} å¯¹é«˜ç›¸ä¼¼åº¦é‚®ä»¶ (ç›¸ä¼¼åº¦ > {threshold}):")
                for pair in high_similarity_pairs[:10]:  # åªæ˜¾ç¤ºå‰10å¯¹
                    print(f"  ç›¸ä¼¼åº¦: {pair['similarity']:.3f}")
                    print(f"    [{pair['label1']}] {pair['subject1']}...")
                    print(f"    [{pair['label2']}] {pair['subject2']}...")
                    if pair['label1'] != pair['label2']:
                        print(f"    âš ï¸  æ ‡ç­¾ä¸ä¸€è‡´!")
                    print()
            else:
                print(f"âœ… æœªå‘ç°é«˜ç›¸ä¼¼åº¦é‚®ä»¶å¯¹")
            
            # ç±»å†…ç›¸ä¼¼åº¦ç»Ÿè®¡
            print(f"ğŸ“Š å„ç±»åˆ«å†…éƒ¨ç›¸ä¼¼åº¦:")
            for label in self.df['label'].unique():
                label_indices = self.df[self.df['label'] == label].index
                if len(label_indices) > 1:
                    label_similarities = []
                    for i in range(len(label_indices)):
                        for j in range(i + 1, len(label_indices)):
                            idx1, idx2 = label_indices[i], label_indices[j]
                            label_similarities.append(similarity_matrix[idx1][idx2])
                    
                    if label_similarities:
                        avg_similarity = np.mean(label_similarities)
                        print(f"  {label}: {avg_similarity:.3f}")
            
        except Exception as e:
            print(f"âŒ ç›¸ä¼¼åº¦åˆ†æå¤±è´¥: {e}")
    
    def cross_validation_preview(self):
        """äº¤å‰éªŒè¯é¢„è§ˆ"""
        print("\nğŸ¯ æ¨¡å‹æ€§èƒ½é¢„æµ‹")
        print("=" * 60)
        
        try:
            from sklearn.model_selection import cross_val_score, StratifiedKFold
            from sklearn.linear_model import LogisticRegression
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.pipeline import Pipeline
            
            # å‡†å¤‡æ•°æ®
            X = self.df['full_text']
            y = self.df['label']
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œäº¤å‰éªŒè¯
            min_class_count = y.value_counts().min()
            if min_class_count < 2:
                print(f"âš ï¸  æŸäº›ç±»åˆ«æ ·æœ¬æ•°è¿‡å°‘ (æœ€å°‘: {min_class_count}), æ— æ³•è¿›è¡Œå¯é çš„äº¤å‰éªŒè¯")
                print("å»ºè®®æ¯ä¸ªç±»åˆ«è‡³å°‘æœ‰5ä¸ªæ ·æœ¬")
                return
            
            # åˆ›å»ºpipeline
            pipeline = Pipeline([
                ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 2))),
                ('classifier', LogisticRegression(max_iter=1000, random_state=42))
            ])
            
            # äº¤å‰éªŒè¯
            cv_folds = min(5, min_class_count)  # ç¡®ä¿foldæ•°ä¸è¶…è¿‡æœ€å°ç±»åˆ«æ ·æœ¬æ•°
            cv_scores = cross_val_score(
                pipeline, X, y, 
                cv=StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42),
                scoring='accuracy'
            )
            
            print(f"ğŸ“Š {cv_folds}æŠ˜äº¤å‰éªŒè¯ç»“æœ:")
            print(f"  å¹³å‡å‡†ç¡®ç‡: {cv_scores.mean():.3f} (Â±{cv_scores.std()*2:.3f})")
            print(f"  å„æŠ˜å¾—åˆ†: {[f'{score:.3f}' for score in cv_scores]}")
            
            # æ€§èƒ½è¯„ä¼°
            if cv_scores.mean() > 0.85:
                print(f"  âœ… æ¨¡å‹æ€§èƒ½ä¼˜ç§€")
            elif cv_scores.mean() > 0.75:
                print(f"  âœ… æ¨¡å‹æ€§èƒ½è‰¯å¥½")
            elif cv_scores.mean() > 0.65:
                print(f"  âš ï¸  æ¨¡å‹æ€§èƒ½ä¸€èˆ¬ï¼Œå»ºè®®å¢åŠ æ›´å¤šè®­ç»ƒæ•°æ®")
            else:
                print(f"  âŒ æ¨¡å‹æ€§èƒ½è¾ƒå·®ï¼Œéœ€è¦å¤§é‡æ”¹è¿›è®­ç»ƒæ•°æ®")
            
        except ImportError:
            print("âŒ ç¼ºå°‘scikit-learnåº“ï¼Œæ— æ³•è¿›è¡Œæ¨¡å‹è¯„ä¼°")
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
    
    def generate_improvement_suggestions(self):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        print("\nğŸ’¡ æ•°æ®æ”¹è¿›å»ºè®®")
        print("=" * 60)
        
        suggestions = []
        
        # æ£€æŸ¥æ•°æ®é‡
        total_samples = len(self.df)
        if total_samples < 200:
            suggestions.append(f"ğŸ“ˆ å¢åŠ æ€»ä½“æ•°æ®é‡ (å½“å‰: {total_samples}, å»ºè®®: 200+)")
        
        # æ£€æŸ¥ç±»åˆ«å¹³è¡¡
        label_counts = self.df['label'].value_counts()
        min_samples = label_counts.min()
        max_samples = label_counts.max()
        
        if min_samples < 30:
            suggestions.append(f"âš–ï¸  å¢åŠ å°‘æ•°ç±»åˆ«æ ·æœ¬ (æœ€å°‘ç±»åˆ«ä»…æœ‰ {min_samples} ä¸ªæ ·æœ¬)")
        
        if max_samples / min_samples > 3:
            suggestions.append(f"âš–ï¸  å¹³è¡¡å„ç±»åˆ«æ•°æ®åˆ†å¸ƒ (æ¯”ä¾‹å·®å¼‚: {max_samples/min_samples:.1f}:1)")
        
        # æ£€æŸ¥æ–‡æœ¬è´¨é‡
        short_texts = (self.df['text_length'] < 20).sum()
        if short_texts > 0:
            suggestions.append(f"ğŸ“ æ”¹è¿›çŸ­æ–‡æœ¬è´¨é‡ ({short_texts} æ¡é‚®ä»¶è¿‡çŸ­)")
        
        # æ£€æŸ¥é‡å¤æ•°æ®
        duplicates = self.df.duplicated(['subject', 'body']).sum()
        if duplicates > 0:
            suggestions.append(f"ğŸ—‘ï¸  åˆ é™¤é‡å¤é‚®ä»¶ ({duplicates} æ¡)")
        
        # è¾“å‡ºå»ºè®®
        if suggestions:
            print("ä¼˜å…ˆæ”¹è¿›é¡¹:")
            for i, suggestion in enumerate(suggestions, 1):
                print(f"  {i}. {suggestion}")
        else:
            print("âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— æ˜æ˜¾æ”¹è¿›éœ€æ±‚")
        
        # å…·ä½“è¡ŒåŠ¨å»ºè®®
        print(f"\nğŸ¯ å…·ä½“è¡ŒåŠ¨å»ºè®®:")
        print(f"  1. ä½¿ç”¨ expand_training_data.py æ‰‹åŠ¨æ·»åŠ æ›´å¤šæ ·æœ¬")
        print(f"  2. ä½¿ç”¨ gmail_importer.py ä»çœŸå®é‚®ä»¶å¯¼å…¥")
        print(f"  3. ä½¿ç”¨ email_annotator.py é‡æ–°æ£€æŸ¥å’Œä¿®æ­£æ ‡ç­¾")
        print(f"  4. æ”¶é›†æ›´å¤šçœŸå®çš„æ±‚èŒé‚®ä»¶è¿›è¡Œæ ‡æ³¨")
    
    def export_quality_report(self):
        """å¯¼å‡ºè´¨é‡æŠ¥å‘Š"""
        print("\nğŸ“„ å¯¼å‡ºæ•°æ®è´¨é‡æŠ¥å‘Š")
        print("=" * 60)
        
        try:
            report = {
                'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
                'total_samples': len(self.df),
                'unique_labels': self.df['label'].nunique(),
                'label_distribution': self.df['label'].value_counts().to_dict(),
                'duplicates': self.df.duplicated(['subject', 'body']).sum(),
                'missing_values': self.df.isnull().sum().to_dict(),
                'text_length_stats': {
                    'mean': float(self.df['text_length'].mean()),
                    'median': float(self.df['text_length'].median()),
                    'min': int(self.df['text_length'].min()),
                    'max': int(self.df['text_length'].max())
                }
            }
            
            import json
            with open('data_quality_report.json', 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… è´¨é‡æŠ¥å‘Šå·²ä¿å­˜åˆ°: data_quality_report.json")
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºæŠ¥å‘Šå¤±è´¥: {e}")
    
    def main_menu(self):
        """ä¸»èœå•"""
        while True:
            print(f"\nğŸ” è®­ç»ƒæ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·")
            print("=" * 60)
            print(f"å½“å‰æ•°æ®: {len(self.df)} æ¡é‚®ä»¶")
            
            print(f"\nè¯·é€‰æ‹©åˆ†æç±»å‹:")
            print("  1. åŸºç¡€ç»Ÿè®¡åˆ†æ")
            print("  2. æ ‡ç­¾åˆ†å¸ƒåˆ†æ")
            print("  3. æ–‡æœ¬è´¨é‡åˆ†æ")
            print("  4. ç›¸ä¼¼åº¦åˆ†æ")
            print("  5. æ¨¡å‹æ€§èƒ½é¢„æµ‹")
            print("  6. ç”Ÿæˆæ”¹è¿›å»ºè®®")
            print("  7. å¯¼å‡ºè´¨é‡æŠ¥å‘Š")
            print("  8. å®Œæ•´åˆ†ææŠ¥å‘Š")
            print("  9. é‡æ–°åŠ è½½æ•°æ®")
            print("  0. é€€å‡º")
            
            choice = input(f"\nè¯·è¾“å…¥é€‰é¡¹ (0-9): ").strip()
            
            if choice == '1':
                self.basic_statistics()
            elif choice == '2':
                self.label_distribution_analysis()
            elif choice == '3':
                self.text_quality_analysis()
            elif choice == '4':
                self.similarity_analysis()
            elif choice == '5':
                self.cross_validation_preview()
            elif choice == '6':
                self.generate_improvement_suggestions()
            elif choice == '7':
                self.export_quality_report()
            elif choice == '8':
                # å®Œæ•´åˆ†æ
                print(f"\nğŸ” æ‰§è¡Œå®Œæ•´æ•°æ®è´¨é‡åˆ†æ...")
                self.basic_statistics()
                self.label_distribution_analysis()
                self.text_quality_analysis()
                self.similarity_analysis()
                self.cross_validation_preview()
                self.generate_improvement_suggestions()
            elif choice == '9':
                self.load_data()
            elif choice == '0':
                print("ğŸ‘‹ å†è§!")
                break
            else:
                print("âŒ æ— æ•ˆé€‰é¡¹!")

def main():
    """ä¸»ç¨‹åº"""
    checker = DataQualityChecker()
    checker.main_menu()

if __name__ == '__main__':
    main()